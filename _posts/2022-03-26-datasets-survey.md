---
title: æœºå™¨å­¦ä¹ ä¸­datasetsçš„è®¾è®¡
tags: ['å¤§æ¨¡å‹è®­ç»ƒ', 'NLP', 'æœºå™¨å­¦ä¹ ', 'æ•°æ®é›†']
key: 20220326
---

åœ¨æœºå™¨å­¦ä¹ ä¸­ä¸€äº›å¸¸ç”¨çš„datasetsåº“çš„è®¾è®¡åŠä½¿ç”¨

<!--more-->

åœ¨æœºå™¨å­¦ä¹ çš„ä½¿ç”¨ä¸­ï¼Œæˆ‘ä»¬å¸¸ç”¨çš„æœ‰å¾ˆå¤šåº“æ¥è¾…åŠ©æ•°æ®çš„å¤„ç†ã€æ•°æ®é›†çš„ç®¡ç†ç­‰ç­‰ï¼Œæ¯”å¦‚è¯´pandasã€tensorflowç­‰ç­‰ï¼Œå› ä¸ºæœ€è¿‘åœ¨åšNLPç›¸å…³çš„ï¼Œæ‰€ä»¥ç€é‡äº†è§£ä¸€ä¸‹HuggingFaceçš„Datasetã€‚

[Github](https://github.com/huggingface/datasets#main-differences-between-ğŸ¤—Datasets-and-tfds)

[Doc](https://huggingface.co/docs/datasets/index)

HuggingFaceçš„datasetsçš„åº•å±‚ä¾èµ–äºApache Arrowï¼Œè€Œtfä¾èµ–äºtf.recordï¼Œè¿™ä¸¤ç§éƒ½å¯ä»¥å¤„ç†å¤§é‡çš„æ•°æ®ï¼Œåˆ©ç”¨å†…å­˜æ˜ å°„ä»ç£ç›˜ä¸Šè¯»å–æ•°æ®ã€‚


### å¦‚ä½•ä½¿ç”¨

å’±ä»¬å…ˆæ¥çœ‹çœ‹hf-datasetsä¸€èˆ¬æ˜¯å¦‚ä½•ä½¿ç”¨çš„ï¼š

```
# åˆ©ç”¨list_datasets()å‡½æ•°ï¼Œæˆ‘ä»¬èƒ½çœ‹åˆ°æ‰€æœ‰huggingfaceæ”¯æŒçš„æ•°æ®é›†ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥åœ¨ç½‘ç«™ä¸Šæ‰¾åˆ°ï¼Œ https://huggingface.co/datasets
import datasets
print(datasets.list_datasets())
```

![](2022-03-26-datasets-survey/2022-03-26-17-27-41.png)

æˆ‘ä»¬ä»é‡Œé¢é€‰ä¸€ä¸ªæ•°æ®é›†å‡ºæ¥çœ‹çœ‹é‡Œé¢æ˜¯å¦‚ä½•å­˜å‚¨çš„ï¼Œè¿™é‡Œæ‹¿ag_newsè¯•è¯•

é€šè¿‡load_datasetï¼Œä¼ å…¥æ•°æ®é›†çš„åå­—å°±å¯ä»¥å¿«é€Ÿè¯»å–æ•°æ®é›†ï¼ŒåŒæ—¶åŒ…æ‹¬è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

![](2022-03-26-datasets-survey/2022-03-26-17-50-32.png)

datasetæœ‰å„ç§å±æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ä»è¿™ä¸ªç»“æ„é‡Œé¢è·å–ä»»ä½•æˆ‘ä»¬æƒ³è¦çš„ä¿¡æ¯

![](2022-03-26-datasets-survey/2022-03-26-17-53-52.png)

ä¹Ÿå¯ä»¥çœ‹åˆ°é‡Œé¢çš„åŸå§‹æ•°æ®é•¿ä»€ä¹ˆæ ·

![](2022-03-26-datasets-survey/2022-03-26-18-29-04.png)

### æ•°æ®å¤„ç†

æ¯”å¦‚è¯´æˆ‘ç°åœ¨éœ€è¦è®­ç»ƒä¸€ä¸ªBertï¼Œæ€ä¹ˆæŠŠè¿™ä¸ªdatasetsç”¨èµ·æ¥å‘¢ã€‚

```
from datasets import load_dataset
from transformers import AutoTokenizer
raw_ds = load_dataset('wikitext', 'wikitext-2-v1')

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

def tokenize_function(examples):
    return tokenizer(examples['text'], truncation=True, padding="max_length", max_length=512, return_special_tokens_mask=True)

# åŸå§‹ä»£ç æä¾›mapå‡½æ•°ï¼Œå¯ä»¥ç›´æ¥å¯¹é‡Œé¢çš„æ¯ä¸€ä¸ªå…ƒç´ è¿‡ä¸€ä¸ªå‡½æ•°ï¼Œåˆ©ç”¨è¿™ä¸ªæ–¹æ³•å¯ä»¥å¤šçº¿ç¨‹å¤„ç†æ•°æ®
tokenized_datasets = raw_ds.map(
    tokenize_function,
    batched=True,
    num_proc=2,
    remove_columns=raw_ds['train'].column_names,
    desc="Running tokenizer on every text in dataset",
)

print(tokenized_datasets)
```

è¿è¡Œä¸Šé¢çš„ä»£ç å¯ä»¥å¾—åˆ°ç»“æœ

![](2022-03-26-datasets-survey/2022-03-27-00-03-37.png)

å¯ä»¥çœ‹åˆ°ç›´æ¥ç”Ÿæˆäº†æœ€åçš„samplesï¼Œä½¿ç”¨èµ·æ¥å¾ˆæ–¹ä¾¿

### æºç è§£æ

ç•™å‘å¾…å¡«


